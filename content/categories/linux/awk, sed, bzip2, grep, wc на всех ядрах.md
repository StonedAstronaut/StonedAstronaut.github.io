title: awk, sed, bzip2, grep, wc на всех ядрах
date: 2015-08-03 17:32:00
status: published
tags: linux, grep, awk, sed, bzip2, parallel

Существует такая проблема: вы хотите добавить огромный список (сотни мегабайт) или запустить поиск по регулярному выражению (**grep**) на нем, или совершить какую-то еще трудно распараллеливаемую операцию. У вас, вероятно, есть четыре ядра или даже больше, но наши проверенные и надежные инструменты **grep**, **bzip2**, **wc**, **awk**, **sed** и т.д. однопоточные и будут использовать всего одно ядро. Как сказал бы Картман, «Как мне добраться до этих ядер?» Давайте использовать все четыре ядра нашей Linux-машины с помощью **GNU Parallel**, небольшой map-reduce магии и малоизвестного параметра `--pipes` (также известного как `--spreadstdin`). Поверьте, удовольствие от работы прямо пропорционально количеству задействованных ядер!

[TOC]

### BZIP2

**bzip2** сжимает лучше, чем **gzip**, но он такой медленый! К счатью, эта проблема решаема. Просто вместо этого:

    :::bash
    cat bigfile.bin | bzip2 --best &gt; compressedfile.bz2

можно написать вот это:

    :::bash
    cat bigfile.bin | parallel --pipe --recend '' -k bzip2 --best &gt; compressedfile.bz2

**GNU Parallel** особенно ускоряет **bzip2** за счет работы на нескольких ядрах. Дайте ему свободу и вы будете вознаграждены.

### GREP

Если у вас имеется огромный текстовый файл для прогона **grep**'ом, то лучше используйте вместо этого:

    :::bash
    grep pattern bigfile.txt

вот это:
    
    :::bash
    cat bigfile.txt | parallel --pipe grep 'pattern'

или это:

    :::bash
    cat bigfile.txt | parallel --block 10M --pipe grep 'pattern'

или это:

    :::bash
    parallel --pipepart -a bigfile.txt --block 30m wc -l | awk '{s+=$1} END {print s}'

Вторая рекомендуемая команда показывает, как использовать команду `--block` с 10**MB** данных из вашего файла. С этим параметром можно поиграться, чтобы понять, какой объем данных вы хотите обрабатывать одним ядром. Вот здесь можно посмотреть, как обрабатывать grep'ом не один файл, а несколько.

### AWK

А вот это пример того, как с помощью **awk** добавить числа в очень большой файл. Вместо этого:

    :::bash
    cat rands20M.txt | awk '{s+=$1} END {print s}'

попробуйте это:

    :::bash
    cat rands20M.txt | parallel --pipe awk \'{s+=\$1} END {print s}\' | awk '{s+=$1} END {print s}'

Здесь `--pipe` позволяет использовать несколько чанков для вызова **awk**, записывая промежуточные результаты, а которые затем сливаются в конечный.

### WC

Хотите запустить супер-распараллеленный подсчет строк в файле? Тогда вместо этого:
    
    :::bash
    wc -l bigfile.txt

запустите вот это:

    :::bash
    cat bigfile.txt | parallel --pipe wc -l | awk '{s+=$1} END {print s}'


Вот это уже здорово. Что происходит? При распараллеленном вызове мы направляем несколько вызовов `wc -l`, формируя промежуточные результаты, а затем вызываем awk.

### SED

Захотелось сделать огромное количество перемещений в большом файле? Тогда забудьте об этом:

    :::bash
    sed s^old^new^g bigfile.txt

и используйте вот это:

    :::bash
    cat bigfile.txt | parallel --pipe sed s^old^new^g

... и затем направьте вывод в ваш любимый файл.

[source](http://www.rankfocus.com/use-cpu-cores-linux-commands/)
